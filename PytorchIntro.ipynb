{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac85b922-30b2-4ad4-83ba-54651f5a51f7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec9a2b45-127f-42f1-943d-0e10d229495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f265ecc-f279-4127-9a9e-4be12ba63baf",
   "metadata": {},
   "source": [
    "## Import Data and Set up Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a774ef4-a0a3-4d91-ac41-840f7ed7a082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000 # Number of training samples out of 50000 total samples\n",
    "\n",
    "# Normalize Data\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # Mean and Std Vectors\n",
    "            ])\n",
    "\n",
    "# Set up a Data Loader + Sampler combination for Batch Training\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd0ad0-ea9d-4e7f-95a4-3bafa2f69481",
   "metadata": {},
   "source": [
    "## Set up Training Device and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53755f8b-ffad-46b3-b5b1-aa5516f856de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8975716-8a9f-44eb-9ef9-73992c1bb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100 # How often we print the train loss\n",
    "dtype = torch.float32 # Data type to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4091583-2f72-4c93-a7d5-efb41c7532b9",
   "metadata": {},
   "source": [
    "## Custom Pytorch NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bf49908-668f-4829-aa9e-eb2fe2435adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening: tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]]],\n",
      "\n",
      "\n",
      "        [[[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]]])\n",
      "After flattening: tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "# Test flatten\n",
    "\n",
    "x = torch.arange(12).view(2, 1, 3, 2)\n",
    "print(f\"Before flattening: {x}\")\n",
    "print(f\"After flattening: {flatten(x)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ce4e5-8c8e-4f65-92f4-bddecdfbb363",
   "metadata": {},
   "source": [
    "### Two-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "073a8267-428b-4b22-9830-684e955d4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural networks; the architecture is:\n",
    "    NN is fully connected -> ReLU -> fully connected layer.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of PyTorch Tensors giving weights for the network;\n",
    "      w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A PyTorch Tensor of shape (N, C) giving classification scores for\n",
    "      the input data x.\n",
    "    \"\"\"\n",
    "    \n",
    "    # first we flatten the image\n",
    "    x = flatten(x)  # shape: [batch_size, C x H x W]\n",
    "    \n",
    "    w1, w2 = params\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    # you can also use `.clamp(min=0)`, equivalent to F.relu()\n",
    "    x = F.relu(x.mm(w1))\n",
    "    x = x.mm(w2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16075dfa-e13d-4ee9-98d7-84ad1990cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test Output (should be [64, 10])\n",
    "\n",
    "hidden_layer_size = 42\n",
    "x = torch.zeros((64, 50), dtype=dtype)  # Minibatch size 64, Feature dimension 50\n",
    "w1 = torch.zeros((50, hidden_layer_size), dtype=dtype)\n",
    "w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype)\n",
    "scores = two_layer_fc(x, [w1, w2])\n",
    "print(scores.size())  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966094d-7228-4af4-8592-97e31e796839",
   "metadata": {},
   "source": [
    "### Three-Layer Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ba3723-8f35-4ca7-9c35-03e4e57cc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    1. A convolutional layer (with bias) with `channel_1` filters, each with shape `KW1 x KH1`, and zero-padding of two\n",
    "    2. ReLU nonlinearity\n",
    "    3. A convolutional layer (with bias) with `channel_2` filters, each with shape `KW2 x KH2`, and zero-padding of one\n",
    "    4. ReLU nonlinearity\n",
    "    5. Fully-connected layer with bias, producing scores for C classes.\n",
    "    \n",
    "    Performs the forward pass of a three-layer convolutional network with the\n",
    "    architecture defined above.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images\n",
    "    - params: A list of PyTorch Tensors giving the weights and biases for the\n",
    "      network; contains the following:\n",
    "      - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights\n",
    "        for the first convolutional layer\n",
    "      - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first\n",
    "        convolutional layer\n",
    "      - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving\n",
    "        weights for the second convolutional layer\n",
    "      - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second\n",
    "        convolutional layer\n",
    "      - fc_w: PyTorch Tensor giving weights for the fully-connected layer\n",
    "      - fc_b: PyTorch Tensor giving biases for the fully-connected layer\n",
    "    \n",
    "    Returns:\n",
    "    - scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "\n",
    "    x = F.conv2d(x, weight=conv_w1, bias=conv_b1, padding=2)\n",
    "    x = F.relu(x)\n",
    "    x = F.conv2d(x, weight=conv_w2, bias=conv_b2, padding=1)\n",
    "    x = F.relu(x)\n",
    "    x = flatten(x)\n",
    "    scores = x.mm(fc_w) + fc_b\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c0170cd-c2c1-4cbd-b9a9-866a51b0e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test Output (should be [64, 10])\n",
    "\n",
    "x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # Minibatch size 64, image size [3, 32, 32]\n",
    "\n",
    "conv_w1 = torch.zeros((6, 3, 5, 5), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n",
    "conv_b1 = torch.zeros((6,))  # out_channel\n",
    "conv_w2 = torch.zeros((9, 6, 3, 3), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n",
    "conv_b2 = torch.zeros((9,))  # out_channel\n",
    "\n",
    "# Calculating the shape of the tensor after two conv layers, before the fully-connected layer\n",
    "fc_w = torch.zeros((9 * 32 * 32, 10))\n",
    "fc_b = torch.zeros(10)\n",
    "\n",
    "scores = three_layer_convnet(x, [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b])\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8650d-53c1-4127-8171-cc771ff180d2",
   "metadata": {},
   "source": [
    "### Custom Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c872210-3368-4592-8393-4119f5e208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weight(shape, device, dtype):\n",
    "    \"\"\"\n",
    "    Create random Tensors for weights\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator. \n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True # Used for backward pass\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape, device, dtype):\n",
    "    \"\"\"\n",
    "    Create Tensors with zeros for weights\n",
    "    \"\"\"\n",
    "    \n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5af442d9-8af4-4cdc-bbec-9acd2b4b610e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9002,  0.3946, -0.0219,  0.6299,  0.5653],\n",
       "        [ 0.6567, -1.3923, -1.4814, -0.0067,  0.5334],\n",
       "        [-0.8200,  1.6313, -0.0149, -0.8094, -0.0763]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight of shape [3 x 5]\n",
    "random_weight((3, 5), device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21a34759-0d71-4300-a683-e4af25cb62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model_fn, params, device, dtype):\n",
    "    \"\"\"\n",
    "    Check the accuracy of a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - loader: A DataLoader for the data split we want to check\n",
    "    - model_fn: A function that performs the forward pass of the model,\n",
    "      with the signature scores = model_fn(x, params)\n",
    "    - params: List of PyTorch Tensors giving parameters of the model\n",
    "    \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    split = 'val' if loader.dataset.train else 'test'\n",
    "    print('Checking accuracy on the %s set' % split)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  \n",
    "            y = y.to(device=device, dtype=torch.int64)\n",
    "            scores = model_fn(x, params)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d87b8689-0918-4e05-be74-cd770c4ccf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_fn, params, learning_rate, device, dtype):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    \n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: compute scores and loss\n",
    "        scores = model_fn(x, params)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters. We don't want to backpropagate through the\n",
    "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "        # context manager to prevent a computational graph from being built.\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # Manually zero the gradients after running the backward pass\n",
    "                w.grad.zero_()\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            check_accuracy(loader_val, model_fn, params, device, dtype)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb048b-bc74-4e05-9194-74682bcf0983",
   "metadata": {},
   "source": [
    "### Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "874fb10f-688c-41f1-ac21-955d1b246223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.7549\n",
      "Checking accuracy on the val set\n",
      "Got 140 / 1000 correct (14.00%)\n",
      "\n",
      "Iteration 100, loss = 2.2964\n",
      "Checking accuracy on the val set\n",
      "Got 367 / 1000 correct (36.70%)\n",
      "\n",
      "Iteration 200, loss = 1.9493\n",
      "Checking accuracy on the val set\n",
      "Got 370 / 1000 correct (37.00%)\n",
      "\n",
      "Iteration 300, loss = 1.7003\n",
      "Checking accuracy on the val set\n",
      "Got 409 / 1000 correct (40.90%)\n",
      "\n",
      "Iteration 400, loss = 2.1938\n",
      "Checking accuracy on the val set\n",
      "Got 352 / 1000 correct (35.20%)\n",
      "\n",
      "Iteration 500, loss = 1.8675\n",
      "Checking accuracy on the val set\n",
      "Got 433 / 1000 correct (43.30%)\n",
      "\n",
      "Iteration 600, loss = 1.9911\n",
      "Checking accuracy on the val set\n",
      "Got 391 / 1000 correct (39.10%)\n",
      "\n",
      "Iteration 700, loss = 1.5363\n",
      "Checking accuracy on the val set\n",
      "Got 432 / 1000 correct (43.20%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the Two-Layer Net (No hyperparameter tuning)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# We need to explicitly allocate tensors for the fully connected weights, `w1` and `w2`\n",
    "w1 = random_weight((3 * 32 * 32, hidden_layer_size), device, dtype)\n",
    "w2 = random_weight((hidden_layer_size, 10), device, dtype)\n",
    "\n",
    "train_model(two_layer_fc, [w1, w2], learning_rate, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13a9e7da-2660-4162-8db4-d64045f336ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 5.0166\n",
      "Checking accuracy on the val set\n",
      "Got 124 / 1000 correct (12.40%)\n",
      "\n",
      "Iteration 100, loss = 2.0025\n",
      "Checking accuracy on the val set\n",
      "Got 351 / 1000 correct (35.10%)\n",
      "\n",
      "Iteration 200, loss = 1.5985\n",
      "Checking accuracy on the val set\n",
      "Got 402 / 1000 correct (40.20%)\n",
      "\n",
      "Iteration 300, loss = 1.4957\n",
      "Checking accuracy on the val set\n",
      "Got 436 / 1000 correct (43.60%)\n",
      "\n",
      "Iteration 400, loss = 1.4707\n",
      "Checking accuracy on the val set\n",
      "Got 448 / 1000 correct (44.80%)\n",
      "\n",
      "Iteration 500, loss = 1.6312\n",
      "Checking accuracy on the val set\n",
      "Got 460 / 1000 correct (46.00%)\n",
      "\n",
      "Iteration 600, loss = 1.5946\n",
      "Checking accuracy on the val set\n",
      "Got 463 / 1000 correct (46.30%)\n",
      "\n",
      "Iteration 700, loss = 1.4478\n",
      "Checking accuracy on the val set\n",
      "Got 477 / 1000 correct (47.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the Three-Layer ConvNet (No hyperparameter tuning)\n",
    "\n",
    "# 1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n",
    "# 2. ReLU\n",
    "# 3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n",
    "# 4. ReLU\n",
    "# 5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "learning_rate = 3e-3\n",
    "\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "conv_w1 = random_weight((channel_1, 3, 5, 5), device, dtype)\n",
    "conv_b1 = random_weight((channel_1,), device, dtype)\n",
    "conv_w2 = random_weight((channel_2, channel_1, 3, 3), device, dtype)\n",
    "conv_b2 = random_weight((channel_2,), device, dtype)\n",
    "fc_w = random_weight((channel_2*32*32, 10), device, dtype)\n",
    "fc_b = random_weight((10,), device, dtype)\n",
    "\n",
    "params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "train_model(three_layer_convnet, params, learning_rate, device, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3aa19b-8c5f-45ed-89f1-0f96937a488c",
   "metadata": {},
   "source": [
    "## Pytorch Module Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f059a8d-5121-4f29-91a0-bad036497361",
   "metadata": {},
   "source": [
    "### Two-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20c2b2df-8d44-44b7-adae-dfb053770fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # nn.init package contains convenient initialization methods\n",
    "        # http://pytorch.org/docs/master/nn.html#torch-nn-init \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        x = flatten(x)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05ca0a38-a98c-4975-a933-9c13f0cca1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test Output (should be [64, 10])\n",
    "\n",
    "input_size = 50\n",
    "x = torch.zeros((64, input_size), dtype=dtype)  # Minibatch size 64, feature dimension 50\n",
    "model = TwoLayerFC(input_size, 42, 10)\n",
    "scores = model(x)\n",
    "print(scores.size())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ede47-6cec-40c6-b299-313619339df2",
   "metadata": {},
   "source": [
    "### Three-Layer Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d9c0a2b-075e-45cb-ab34-e14dfe1ddb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2, num_classes):\n",
    "        \"\"\"\n",
    "        1. Convolutional layer with `channel_1` 5x5 filters with zero-padding of 2\n",
    "        2. ReLU\n",
    "        3. Convolutional layer with `channel_2` 3x3 filters with zero-padding of 1\n",
    "        4. ReLU\n",
    "        5. Fully-connected layer to `num_classes` classes\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "       \n",
    "        self.fc1 = nn.Conv2d(in_channels=3, out_channels=channel_1, kernel_size=5, padding=2)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight) # Same initialization as before\n",
    "        self.fc2 = nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=3, padding=1)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)        \n",
    "        self.fc3 = nn.Linear(channel_2*32*32, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight) \n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        \n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        x = flatten(x)\n",
    "        scores = self.fc3(x)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "778f3e23-a1d2-45f2-b03b-d3435b9e447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test Output (should be [64, 10])\n",
    "\n",
    "x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]\n",
    "model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)\n",
    "scores = model(x)\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa927a4-f130-4832-9315-1249efa5b46f",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd5a260d-b4b0-4d39-8845-d85aeb23cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_module(loader, model, device, dtype):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a6a76f0-f87e-42b9-9bc1-68292b9039dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_module(model, optimizer, device, dtype, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_module(loader_val, model, device, dtype)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4293ff6-d71a-4d95-9a93-5ec3242dca11",
   "metadata": {},
   "source": [
    "### Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbdfd20c-cda6-4a48-8b3c-6ea2f65874e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.6557\n",
      "Checking accuracy on validation set\n",
      "Got 160 / 1000 correct (16.00)\n",
      "\n",
      "Iteration 100, loss = 2.3441\n",
      "Checking accuracy on validation set\n",
      "Got 313 / 1000 correct (31.30)\n",
      "\n",
      "Iteration 200, loss = 2.1471\n",
      "Checking accuracy on validation set\n",
      "Got 371 / 1000 correct (37.10)\n",
      "\n",
      "Iteration 300, loss = 2.3162\n",
      "Checking accuracy on validation set\n",
      "Got 382 / 1000 correct (38.20)\n",
      "\n",
      "Iteration 400, loss = 2.0473\n",
      "Checking accuracy on validation set\n",
      "Got 381 / 1000 correct (38.10)\n",
      "\n",
      "Iteration 500, loss = 1.6748\n",
      "Checking accuracy on validation set\n",
      "Got 452 / 1000 correct (45.20)\n",
      "\n",
      "Iteration 600, loss = 1.7866\n",
      "Checking accuracy on validation set\n",
      "Got 435 / 1000 correct (43.50)\n",
      "\n",
      "Iteration 700, loss = 1.8146\n",
      "Checking accuracy on validation set\n",
      "Got 373 / 1000 correct (37.30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the Two-Layer Net (No hyperparameter tuning)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model_module(model, optimizer, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b78b9baa-0a5a-4199-8328-cce0cdcf2e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.7216\n",
      "Checking accuracy on validation set\n",
      "Got 113 / 1000 correct (11.30)\n",
      "\n",
      "Iteration 100, loss = 1.9634\n",
      "Checking accuracy on validation set\n",
      "Got 335 / 1000 correct (33.50)\n",
      "\n",
      "Iteration 200, loss = 1.8439\n",
      "Checking accuracy on validation set\n",
      "Got 400 / 1000 correct (40.00)\n",
      "\n",
      "Iteration 300, loss = 1.7141\n",
      "Checking accuracy on validation set\n",
      "Got 421 / 1000 correct (42.10)\n",
      "\n",
      "Iteration 400, loss = 1.7226\n",
      "Checking accuracy on validation set\n",
      "Got 436 / 1000 correct (43.60)\n",
      "\n",
      "Iteration 500, loss = 1.9288\n",
      "Checking accuracy on validation set\n",
      "Got 453 / 1000 correct (45.30)\n",
      "\n",
      "Iteration 600, loss = 1.7896\n",
      "Checking accuracy on validation set\n",
      "Got 465 / 1000 correct (46.50)\n",
      "\n",
      "Iteration 700, loss = 1.5000\n",
      "Checking accuracy on validation set\n",
      "Got 448 / 1000 correct (44.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the Three-Layer ConvNet (No hyperparameter tuning)\n",
    "\n",
    "learning_rate = 3e-3\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "model = ThreeLayerConvNet(in_channel=3, channel_1=channel_1, channel_2=channel_2, num_classes=10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train_model_module(model, optimizer, device, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994aefc8-1b64-42be-a9ad-7f63dd809ea4",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d33d7c98-ca17-42ad-ae54-7f396307c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.2929\n",
      "Checking accuracy on validation set\n",
      "Got 120 / 1000 correct (12.00)\n",
      "\n",
      "Iteration 100, loss = 2.0204\n",
      "Checking accuracy on validation set\n",
      "Got 334 / 1000 correct (33.40)\n",
      "\n",
      "Iteration 200, loss = 1.6405\n",
      "Checking accuracy on validation set\n",
      "Got 409 / 1000 correct (40.90)\n",
      "\n",
      "Iteration 300, loss = 1.4548\n",
      "Checking accuracy on validation set\n",
      "Got 426 / 1000 correct (42.60)\n",
      "\n",
      "Iteration 400, loss = 1.5407\n",
      "Checking accuracy on validation set\n",
      "Got 476 / 1000 correct (47.60)\n",
      "\n",
      "Iteration 500, loss = 1.5166\n",
      "Checking accuracy on validation set\n",
      "Got 492 / 1000 correct (49.20)\n",
      "\n",
      "Iteration 600, loss = 1.6643\n",
      "Checking accuracy on validation set\n",
      "Got 503 / 1000 correct (50.30)\n",
      "\n",
      "Iteration 700, loss = 1.5494\n",
      "Checking accuracy on validation set\n",
      "Got 519 / 1000 correct (51.90)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "    \n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "# Same Three-Layer ConvNet Model\n",
    "model = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=channel_1, kernel_size=5, padding=2), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=3, padding=1),\n",
    "                      nn.ReLU(), \n",
    "                      Flatten(),\n",
    "                      nn.Linear(channel_2*32*32, 10)\n",
    "        )\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model_module(model, optimizer, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee808fc-d606-4004-9faa-2515e25bd32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
